{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install laspy\n",
    "!pip install rasterio\n",
    "!pip install alphashape shapely\n",
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import tqdm\n",
    "import geopandas as gpd\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, RocCurveDisplay\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import laspy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/kaggle/input/hutechaichallenge2024-mc/Train/*/*.las\")\n",
    "file_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"path\": file_paths})\n",
    "data[\"species\"] = data[\"path\"].map(lambda p: p.split(\"/\")[-2])\n",
    "data.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the highest tree (Ptop)\n",
    "def find_highest_tree(points):\n",
    "    idx = np.argmax(points[:, 2])  # Find point with max Z\n",
    "    return points[idx]\n",
    "\n",
    "# Step 2: Rotate profile and extract edge points\n",
    "def extract_tree_edges(points, Ptop, num_segments=36):\n",
    "    angles = np.linspace(0, np.pi, num_segments)  # Rotate from 0¬∞ to 180¬∞\n",
    "    edge_points = []\n",
    "    \n",
    "    for angle in angles:\n",
    "        # Rotate points around Ptop\n",
    "        rot_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "        rotated_xy = (points[:, :2] - Ptop[:2]) @ rot_matrix.T + Ptop[:2]\n",
    "        rotated_points = np.column_stack((rotated_xy, points[:, 2]))\n",
    "        \n",
    "        # Find the convex hull\n",
    "        hull = ConvexHull(rotated_points[:, :2])\n",
    "        edge_points.append(rotated_points[hull.vertices])\n",
    "    \n",
    "    return edge_points\n",
    "\n",
    "# Step 3: Cluster trees based on extracted edges\n",
    "def segment_trees(points, edge_points, threshold=2.0):\n",
    "    clustered = np.zeros(len(points), dtype=int)\n",
    "    cluster_id = 1\n",
    "    \n",
    "    for edges in edge_points:\n",
    "        for edge in edges:\n",
    "            distances = np.linalg.norm(points[:, :2] - edge[:2], axis=1)\n",
    "            clustered[distances < threshold] = cluster_id\n",
    "        cluster_id += 1\n",
    "    \n",
    "    return clustered.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tree_crown_keypoints(points, tree_top, dis):\n",
    "    \"\"\"\n",
    "    Extract keypoints of the tree crown based on the steps provided.\n",
    "\n",
    "    Args:\n",
    "        points (numpy.ndarray): Array of points (N, 3) representing the point cloud.\n",
    "        tree_top (tuple): (x, y, z) coordinates of the tree top vertex.\n",
    "        dis (float): Average point spacing distance.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Keypoints of the tree crown.\n",
    "    \"\"\"\n",
    "    # 1. Draw the vertical line L from the tree top to the ground\n",
    "    x_top, y_top, z_top = tree_top\n",
    "    z_min = np.min(points[:, 2])  # Ground level (minimum Z value)\n",
    "    \n",
    "    # 2. Calculate the width D\n",
    "    D = 2 * dis  # Width of the profile around the tree top\n",
    "    \n",
    "    # 3. Draw parallel lines with spacing d\n",
    "    d = dis  # Can use the same value for spacing\n",
    "    keypoints = []\n",
    "\n",
    "    # Iterate through several levels from tree top to ground\n",
    "    num_lines = int((z_top - z_min) / d)  # Number of lines along the Z-axis\n",
    "    for i in range(num_lines):\n",
    "        z_level = z_top - i * d  # Current level of the parallel line\n",
    "        # Find points around the current Z level within the profile width D\n",
    "        points_at_level = points[(points[:, 2] >= z_level - D / 2) & \n",
    "                                  (points[:, 2] <= z_level + D / 2) & \n",
    "                                  (np.abs(points[:, 0] - x_top) <= D / 2) & \n",
    "                                  (np.abs(points[:, 1] - y_top) <= D / 2)]\n",
    "        \n",
    "        # Collect keypoints\n",
    "        keypoints.append(points_at_level)\n",
    "    \n",
    "    keypoints = np.vstack(keypoints) if len(keypoints) > 0 else np.array([])\n",
    "    return keypoints\n",
    "\n",
    "def plot_tree_crown(points, keypoints, tree_top):\n",
    "    \"\"\"\n",
    "    Plot the points and keypoints of the tree crown.\n",
    "\n",
    "    Args:\n",
    "        points (numpy.ndarray): Original point cloud.\n",
    "        keypoints (numpy.ndarray): Keypoints of the tree crown.\n",
    "        tree_top (tuple): (x, y, z) coordinates of the tree top vertex.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot all points\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=1, label=\"Point Cloud\")\n",
    "    \n",
    "    # Plot keypoints\n",
    "    ax.scatter(keypoints[:, 0], keypoints[:, 1], keypoints[:, 2], color='red', s=10, label=\"Tree Crown Keypoints\")\n",
    "    \n",
    "    # Mark the tree top\n",
    "    ax.scatter(tree_top[0], tree_top[1], tree_top[2], color='green', s=50, label=\"Tree Top\")\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend()\n",
    "    plt.title(\"Tree Crown Keypoints Extraction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphashape\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import open3d as o3d\n",
    "\n",
    "def compute_alpha_shape(point_cloud, alpha=0.1):\n",
    "    \"\"\"\n",
    "    D·ª±ng bi√™n c·ªßa ƒë√°m m√¢y ƒëi·ªÉm b·∫±ng thu·∫≠t to√°n Alpha Shapes.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (numpy.ndarray): ƒê√°m m√¢y ƒëi·ªÉm (N, 3) ch·ª©a (x, y, z).\n",
    "        alpha (float): Gi√° tr·ªã alpha ƒë·ªÉ ƒëi·ªÅu ch·ªânh ƒë·ªô ch·∫∑t ch·∫Ω c·ªßa bi√™n.\n",
    "\n",
    "    Returns:\n",
    "        Polygon ho·∫∑c MultiPolygon: ƒê∆∞·ªùng bi√™n c·ªßa t√°n c√¢y.\n",
    "    \"\"\"\n",
    "    # Ch·ªâ l·∫•y t·ªça ƒë·ªô X, Y (b·ªè Z v√¨ bi√™n l√† 2D)\n",
    "    xy_points = point_cloud[:, :2]\n",
    "\n",
    "    # T·∫°o Alpha Shape\n",
    "    alpha_shape = alphashape.alphashape(xy_points, alpha)\n",
    "\n",
    "    return alpha_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_alpha_shape(point_cloud, alpha_shape):\n",
    "    \"\"\"\n",
    "    V·∫Ω ƒë∆∞·ªùng bi√™n c·ªßa t√°n c√¢y d·ª±a tr√™n Alpha Shapes.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (numpy.ndarray): ƒê√°m m√¢y ƒëi·ªÉm (N, 3).\n",
    "        alpha_shape (Polygon ho·∫∑c MultiPolygon): ƒê∆∞·ªùng bi√™n.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(point_cloud[:, 0], point_cloud[:, 1], s=1, label=\"ƒêi·ªÉm\")\n",
    "    \n",
    "    if isinstance(alpha_shape, Polygon):\n",
    "        x, y = alpha_shape.exterior.xy\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label=\"Alpha Shape\")\n",
    "    elif isinstance(alpha_shape, MultiPolygon):\n",
    "        for poly in alpha_shape.geoms:\n",
    "            x, y = poly.exterior.xy\n",
    "            plt.plot(x, y, 'r-', linewidth=2, label=\"Alpha Shape\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(\"Alpha Shape c·ªßa T√°n C√¢y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def compute_cloud_features(path: str, *, height_threshold: float = 1.0) -> dict:\n",
    "    las = laspy.read(path)\n",
    "    out = {}\n",
    "    \n",
    "    points = np.stack((las.x, las.y, las.z), axis=1)\n",
    "    \n",
    "    height_mask = points[:, 2] > height_threshold\n",
    "    points = points[height_mask]\n",
    "\n",
    "    # ƒê·∫∂c tr∆∞ng t·ªça ƒë·ªô\n",
    "    x_min, x_max = points[:, 0].min(), points[:, 0].max()\n",
    "    y_min, y_max = points[:, 1].min(), points[:, 1].max()\n",
    "    z_min, z_max = points[:, 2].min(), points[:, 2].max()\n",
    "    mean_x, mean_y, mean_z = points[:, 0].mean(), points[:, 1].mean(), points[:, 2].mean()\n",
    "    var_x, var_y, var_z = points[:, 0].var(), points[:, 1].var(), points[:, 2].var()\n",
    "\n",
    "    # Bounding Box (H√¨nh ch·ªØ nh·∫≠t)\n",
    "    width = x_max - x_min\n",
    "    depth = y_max - y_min\n",
    "    height = z_max - z_min\n",
    "\n",
    "    # üìå Shape Ratio\n",
    "    vertical_proportion = float(height) / (width + depth)\n",
    "\n",
    "    # üìå M·∫≠t ƒë·ªô ƒëi·ªÉm\n",
    "    density_xy = len(points) / ((x_max - x_min) * (y_max - y_min))\n",
    "    density_yz = len(points) / ((y_max - y_min) * (z_max - z_min))\n",
    "    density_xyz = len(points) / ((x_max - x_min) * (y_max - y_min) * (z_max - z_min))\n",
    "    point_density_ratio = density_xy / density_xyz  # üìå T·ª∑ l·ªá m·∫≠t ƒë·ªô ƒëi·ªÉm 2D/3D\n",
    "\n",
    "    # üìå ƒê·∫∑c tr∆∞ng chi·ªÅu cao\n",
    "    P10, P25, P50, P75, P95, P99 = np.percentile(points[:, 2], [10, 25, 50, 75, 95, 99])\n",
    "    canopy_height_ratio = (P75 - P50) / (P99 - P25)\n",
    "\n",
    "    # üìå PCA Eigenvalues (ƒê√°nh gi√° h∆∞·ªõng ch√≠nh)\n",
    "    cov_matrix = np.cov(points.T)\n",
    "    eigenvalues = np.linalg.eigvals(cov_matrix)  # T√≠nh eigenvalues\n",
    "    eigenvalue_sum = np.sum(eigenvalues)  # T·ªïng t·∫•t c·∫£ eigenvalues\n",
    "    eigenvalue_ratio = np.max(eigenvalues) / (eigenvalue_sum + 1e-10)  # H∆∞·ªõng ch√≠nh c·ªßa t√°n\n",
    "\n",
    "    # üìå T√≠nh c√°c ƒë·∫∑c tr∆∞ng d·ª±a tr√™n PCA\n",
    "    if eigenvalue_sum > 0:\n",
    "        linearity = eigenvalues[0] / eigenvalue_sum\n",
    "        scatter = eigenvalues[2] / eigenvalue_sum\n",
    "        eigentropy = -np.sum((eigenvalues / eigenvalue_sum) * np.log(eigenvalues / eigenvalue_sum + 1e-10))\n",
    "    else:\n",
    "        linearity = scatter = eigentropy = 0\n",
    "    \n",
    "    omnivariance = (eigenvalues[0] * eigenvalues[1] * eigenvalues[2]) ** (1/3) if np.all(eigenvalues > 0) else 0\n",
    "    sum_of_eigenvalues = eigenvalue_sum\n",
    "    planarity = (eigenvalues[1] - eigenvalues[2]) / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "    sphericity = eigenvalues[2] / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "    symmetry = eigenvalues[1] / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "\n",
    "    # üìå ƒê·∫∑c tr∆∞ng c·∫•u tr√∫c\n",
    "    mean_height = np.mean(points[:, 2]) - z_min\n",
    "    std_height = np.std(points[:, 2])\n",
    "    canopy_ratio = np.sum(points[:, 2] > np.median(points[:, 2])) / len(points)\n",
    "    roughness = np.std(points[:, 2] - mean_z)\n",
    "    normalized_height_ratio = mean_height / (z_max - z_min)\n",
    "\n",
    "    # üìå Convex Hull (Th·ªÉ t√≠ch & Di·ªán t√≠ch)\n",
    "    try:\n",
    "        hull = ConvexHull(points)\n",
    "        convex_hull_volume = hull.volume\n",
    "        convex_hull_area = hull.area\n",
    "        \n",
    "        # üìå T√≠nh b√°n k√≠nh Bounding Sphere (b√°n k√≠nh l·ªõn nh·∫•t t·ª´ centroid ƒë·∫øn ƒëi·ªÉm xa nh·∫•t)\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        distances = np.linalg.norm(points - centroid, axis=1)\n",
    "        bounding_sphere_radius = np.max(distances)\n",
    "        \n",
    "        # üìå T√≠nh Hull Compactness\n",
    "        sphere_volume = (4/3) * np.pi * (bounding_sphere_radius ** 3)\n",
    "        hull_compactness = convex_hull_volume / sphere_volume if sphere_volume > 0 else 0\n",
    "    \n",
    "    except:\n",
    "        convex_hull_volume = 0  # Tr∆∞·ªùng h·ª£p kh√¥ng t√≠nh ƒë∆∞·ª£c\n",
    "        convex_hull_area = 0\n",
    "        hull_compactness = 0\n",
    "\n",
    "    # üìå Vertical Distribution (Ph√¢n b·ªë ƒëi·ªÉm theo Z)\n",
    "    hist_z, _ = np.histogram(points[:, 2], bins=10)\n",
    "    vertical_distribution = np.std(hist_z) / np.mean(hist_z) if np.mean(hist_z) > 0 else 0\n",
    "\n",
    "    # üìå Point Entropy (ƒê·ªô h·ªón lo·∫°n c·ªßa ph√¢n b·ªë ƒëi·ªÉm)\n",
    "    probs = hist_z / np.sum(hist_z) if np.sum(hist_z) > 0 else np.ones(10) / 10\n",
    "    point_entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "\n",
    "    # üìå S·ª± ph√¢n b·ªë kh√¥ng gian c·ªßa c√°c ƒë·ªânh cao nh·∫•t\n",
    "    top_points = points[points[:, 2] > np.percentile(points[:, 2], 90)]\n",
    "    mean_x_top, mean_y_top, mean_z_top = np.mean(top_points, axis=0)\n",
    "    var_x_top, var_y_top, var_z_top = np.var(top_points, axis=0)\n",
    "\n",
    "    # üìå T√≠nh z_entropy\n",
    "    z_probs = np.histogram(points[:, 2], bins=10, density=True)[0]\n",
    "    z_entropy_value = -np.sum(z_probs * np.log(z_probs + 1e-10))\n",
    "    pct_z_above_mean = np.sum(points[:, 2] > mean_z) / len(points) * 100\n",
    "    pct_z_above_2 = np.sum(points[:, 2] > 2) / len(points) * 100\n",
    "\n",
    "    # üìå ƒê·ªô r·ªông trung b√¨nh c·ªßa t√°n c√¢y\n",
    "    tree_top = (mean_x, mean_y, z_max)\n",
    "    crown_keypoints = extract_tree_crown_keypoints(points, tree_top, dis=0.5)\n",
    "    num_crown_keypoints = len(crown_keypoints)\n",
    "    avg_crown_width = np.mean(np.linalg.norm(crown_keypoints[:, :2] - np.array(tree_top[:2]), axis=1)) * 2 if num_crown_keypoints > 0 else 0\n",
    "\n",
    "    # üìå T√≠nh di·ªán t√≠ch bi√™n c·ªßa t√°n c√¢y\n",
    "    alpha_shape = compute_alpha_shape(points, alpha=0.1)\n",
    "    crown_boundary_area = sum(poly.area for poly in alpha_shape.geoms) if isinstance(alpha_shape, MultiPolygon) else (alpha_shape.area if isinstance(alpha_shape, Polygon) else 0)\n",
    "    \n",
    "    # Height distribution features (Woods et al., 2008)\n",
    "    out.update(\n",
    "        {\n",
    "            \"width\": width, \"depth\": depth, \"height\": height,\n",
    "            \"min_z\": z_min, \"max_z\": z_max, \"mean_z\": mean_z, \"var_z\": var_z,\n",
    "            \"vertical_proportion\": vertical_proportion,\n",
    "            \"density_xy\": density_xy, \"density_yz\": density_yz, \"density_xyz\": density_xyz,\n",
    "            \"P10\": P10, \"P25\": P25, \"P50\": P50, \"P75\": P75, \"P95\": P95, \"P99\": P99, \"canopy_height_ratio\": canopy_height_ratio,\n",
    "            \"eigenvalue_1\": eigenvalues[0], \"eigenvalue_2\": eigenvalues[1], \"eigenvalue_3\": eigenvalues[2], \"eigenvalue_sum\": eigenvalue_sum, \"eigenvalue_ratio\": eigenvalue_ratio,\n",
    "            \"linearity\": linearity, \"scatter\": scatter, \"omnivariance\": omnivariance, \"eigentropy\": eigentropy, \"sum_of_eigenvalues\": sum_of_eigenvalues, \"planarity\": planarity, \"sphericity\": sphericity, \"symmetry\": symmetry,\n",
    "            \"mean_height\": mean_height, \"std_height\": std_height, \"roughness\": roughness,\n",
    "            \"convex_hull_volume\": convex_hull_volume, \"convex_hull_area\": convex_hull_area,\n",
    "            \"vertical_distribution\": vertical_distribution, \"point_entropy\": point_entropy,\n",
    "            \"top_points\": top_points, \"mean_x_top\": mean_x_top, \"mean_y_top\": mean_y_top, \"mean_z_top\": mean_z_top,\n",
    "            \"var_x_top\": var_x_top, \"var_y_top\": var_y_top, \"var_z_top\": var_z_top,\n",
    "            \"z_entropy_value\": z_entropy_value, \"pct_z_above_mean\": pct_z_above_mean, \"pct_z_above_2\": pct_z_above_2,\n",
    "            \"avg_crown_width\": avg_crown_width, \"crown_boundary_area\": crown_boundary_area, \"alpha_shape\": alpha_shape\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.json_normalize(data[\"path\"].map(compute_cloud_features))\n",
    "full = pd.concat([data, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full[\"mean_x_top_points\"] = full[\"top_points\"].apply(lambda pts: np.mean([p[0] for p in pts]))\n",
    "full[\"mean_y_top_points\"] = full[\"top_points\"].apply(lambda pts: np.mean([p[1] for p in pts]))\n",
    "full[\"mean_z_top_points\"] = full[\"top_points\"].apply(lambda pts: np.mean([p[2] for p in pts]))\n",
    "\n",
    "full[\"var_x_top_points\"] = full[\"top_points\"].apply(lambda pts: np.var([p[0] for p in pts]))\n",
    "full[\"var_y_top_points\"] = full[\"top_points\"].apply(lambda pts: np.var([p[1] for p in pts]))\n",
    "full[\"var_z_top_points\"] = full[\"top_points\"].apply(lambda pts: np.var([p[2] for p in pts]))\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "full[\"max_top_distance\"] = full[\"top_points\"].apply(lambda pts: np.max(pdist(pts[:, :2])) if len(pts) > 1 else 0)\n",
    "\n",
    "full[\"crown_boundary_area\"] = full[\"alpha_shape\"].apply(lambda poly: poly.area)\n",
    "full[\"crown_perimeter\"] = full[\"alpha_shape\"].apply(lambda poly: poly.length)\n",
    "full[\"crown_compactness\"] = full[\"crown_boundary_area\"] / full[\"crown_perimeter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y ng·∫´u nhi√™n 1 m·∫´u t·ª´ m·ªói species\n",
    "sampled_data = full.groupby(\"species\").sample(n=1, random_state=42)\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_mapping = {\n",
    "    \"Fir\": 1,      # C√¢y l√£nh sam\n",
    "    \"Pine\": 2,     # C√¢y th√¥ng\n",
    "    \"Spruce\": 3,   # C√¢y v√¢n sam\n",
    "    \"Alder\": 4,    # C√¢y trƒÉn\n",
    "    \"Aspen\": 5,    # C√¢y d∆∞∆°ng\n",
    "    \"Birch\": 6,    # C√¢y b·∫°ch d∆∞∆°ng\n",
    "    \"Tilia\": 7     # C√¢y ƒëo·∫°n\n",
    "}\n",
    "\n",
    "full[\"species\"] = full[\"species\"].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_species = full[full[\"species\"].isna()][\"species\"].unique()\n",
    "print(\"Lo√†i ch∆∞a √°nh x·∫°:\", unknown_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full.drop(columns=[\"path\", \"species\"]).copy()\n",
    "y = full.pop(\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=68, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass clssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer tr√≠ch xu·∫•t di·ªán t√≠ch t·ª´ alpha_shape\n",
    "class AlphaShapeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            shape.area if shape is not None else 0  # X·ª≠ l√Ω gi√° tr·ªã None\n",
    "            for shape in X[\"alpha_shape\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# Transformer tr√≠ch xu·∫•t trung b√¨nh ƒë·ªô cao t·ª´ top_points\n",
    "class TopPointsFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([p[2] for p in points]) if isinstance(points, list) and len(points) > 0 else 0\n",
    "            for points in X[\"top_points\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# ColumnTransformer x·ª≠ l√Ω t·∫•t c·∫£ c·ªôt s·ªë + tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ object\n",
    "feature_extractor = ColumnTransformer([\n",
    "    (\"scale_numeric\", StandardScaler(), make_column_selector(dtype_include=np.number)),  # Chu·∫©n h√≥a t·∫•t c·∫£ c·ªôt s·ªë\n",
    "    (\"extract_alpha_shape\", AlphaShapeFeatureExtractor(), [\"alpha_shape\"]),  # Tr√≠ch xu·∫•t t·ª´ alpha_shape\n",
    "    (\"extract_top_points\", TopPointsFeatureExtractor(), [\"top_points\"])  # Tr√≠ch xu·∫•t t·ª´ top_points\n",
    "])\n",
    "\n",
    "# Pipeline v·ªõi Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    (\"feature_extraction\", feature_extractor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=10000, \n",
    "                                      multi_class=\"multinomial\", \n",
    "                                      class_weight=\"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer tr√≠ch xu·∫•t di·ªán t√≠ch t·ª´ alpha_shape\n",
    "class AlphaShapeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            shape.area if shape is not None else 0  # X·ª≠ l√Ω gi√° tr·ªã None\n",
    "            for shape in X[\"alpha_shape\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# Transformer tr√≠ch xu·∫•t trung b√¨nh ƒë·ªô cao t·ª´ top_points\n",
    "class TopPointsFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([p[2] for p in points]) if isinstance(points, list) and len(points) > 0 else 0\n",
    "            for points in X[\"top_points\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# ColumnTransformer x·ª≠ l√Ω t·∫•t c·∫£ c·ªôt s·ªë + tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ object\n",
    "feature_extractor = ColumnTransformer([\n",
    "    (\"scale_numeric\", StandardScaler(), make_column_selector(dtype_include=np.number)),  # Chu·∫©n h√≥a t·∫•t c·∫£ c·ªôt s·ªë\n",
    "    (\"extract_alpha_shape\", AlphaShapeFeatureExtractor(), [\"alpha_shape\"]),  # Tr√≠ch xu·∫•t t·ª´ alpha_shape\n",
    "    (\"extract_top_points\", TopPointsFeatureExtractor(), [\"top_points\"])  # Tr√≠ch xu·∫•t t·ª´ top_points\n",
    "])\n",
    "\n",
    "# Pipeline v·ªõi Logistic Regression\n",
    "pipe_rf = Pipeline([\n",
    "    (\"feature_extraction\", feature_extractor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=1000, \n",
    "                           class_weight=\"balanced\", \n",
    "                           random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# C√°i n√†y l√† gi√° c·ªßa lu·ªìng pipe_lr\n",
    "cv_result_multi = cross_validate(\n",
    "    estimator=pipe_lr,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "scores = cv_result_multi[\"test_score\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(scores, ls=\"\", marker=\".\", c=\"k\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xlabel(\"Cross-validation iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.axhline(scores.mean(), linestyle=\"dashed\", alpha=0.3, c=\"k\")\n",
    "ax.annotate(\n",
    "    text=f\"Accuracy: {scores.mean():.3f} ¬± {scores.std():.3f}\",\n",
    "    xy=(4.5, 0.2),\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    bbox={\n",
    "        \"facecolor\": \"white\",\n",
    "    },\n",
    ")\n",
    "ax.set_title(\"Cross-validation results (multiclass)\")\n",
    "ax.grid(color=\"k\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# C√°i n√†y l√† gi√° c·ªßa lu·ªìng pipe_rf\n",
    "cv_result_multi = cross_validate(\n",
    "    estimator=pipe_rf,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "scores = cv_result_multi[\"test_score\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(scores, ls=\"\", marker=\".\", c=\"k\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xlabel(\"Cross-validation iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.axhline(scores.mean(), linestyle=\"dashed\", alpha=0.3, c=\"k\")\n",
    "ax.annotate(\n",
    "    text=f\"Accuracy: {scores.mean():.3f} ¬± {scores.std():.3f}\",\n",
    "    xy=(4.5, 0.2),\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    bbox={\n",
    "        \"facecolor\": \"white\",\n",
    "    },\n",
    ")\n",
    "ax.set_title(\"Cross-validation results (multiclass)\")\n",
    "ax.grid(color=\"k\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hu·∫•n luy·ªán pipeline\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_multi_lr = pipe_lr.predict(X_test)\n",
    "pred_multi_rf = pipe_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# T√≠nh Accuracy\n",
    "accuracy = accuracy_score(y_test, pred_multi_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# In b√°o c√°o chi ti·∫øt Precision, Recall, F1-score\n",
    "print(classification_report(y_test, pred_multi_lr, digits=4))\n",
    "\n",
    "# V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test,\n",
    "    y_pred=pred_multi_lr,\n",
    "    normalize=\"true\",  # B√¨nh th∆∞·ªùng h√≥a theo h√†ng (gi√° tr·ªã t·ª´ 0 -> 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# T√≠nh Accuracy\n",
    "accuracy = accuracy_score(y_test, pred_multi_rf)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# In b√°o c√°o chi ti·∫øt Precision, Recall, F1-score\n",
    "print(classification_report(y_test, pred_multi_rf, digits=4))\n",
    "\n",
    "# V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test,\n",
    "    y_pred=pred_multi_rf,\n",
    "    normalize=\"true\",  # B√¨nh th∆∞·ªùng h√≥a theo h√†ng (gi√° tr·ªã t·ª´ 0 -> 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K·∫øt h·ª£p 2 m√¥ h√¨nh b·∫±ng VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', pipe_lr),\n",
    "    ('rf', pipe_rf)\n",
    "], voting='soft')  # 'soft' d√πng x√°c su·∫•t d·ª± ƒëo√°n, 'hard' ch·ªâ l·∫•y k·∫øt qu·∫£ ƒëa s·ªë\n",
    "\n",
    "# Train m√¥ h√¨nh\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_file_paths = glob.glob(\"/kaggle/input/hutechaichallenge2024-mc/Test/*/*.las\")\n",
    "test_file_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"path\": test_file_paths})\n",
    "test_data[\"species\"] = test_data[\"path\"].map(lambda p: p.split(\"/\")[-2])\n",
    "test_data.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = pd.json_normalize(test_data[\"path\"].map(compute_cloud_features))\n",
    "full_test = pd.concat([test_data, features_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test[\"mean_x_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.mean([p[0] for p in pts]))\n",
    "full_test[\"mean_y_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.mean([p[1] for p in pts]))\n",
    "full_test[\"mean_z_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.mean([p[2] for p in pts]))\n",
    "\n",
    "full_test[\"var_x_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.var([p[0] for p in pts]))\n",
    "full_test[\"var_y_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.var([p[1] for p in pts]))\n",
    "full_test[\"var_z_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.var([p[2] for p in pts]))\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "full_test[\"max_top_distance\"] = full_test[\"top_points\"].apply(lambda pts: np.max(pdist(pts[:, :2])) if len(pts) > 1 else 0)\n",
    "\n",
    "full_test[\"crown_boundary_area\"] = full_test[\"alpha_shape\"].apply(lambda poly: poly.area)\n",
    "full_test[\"crown_perimeter\"] = full_test[\"alpha_shape\"].apply(lambda poly: poly.length)\n",
    "full_test[\"crown_compactness\"] = full_test[\"crown_boundary_area\"] / full_test[\"crown_perimeter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y ng·∫´u nhi√™n 1 m·∫´u t·ª´ m·ªói species\n",
    "sampled_data_test = full_test.groupby(\"species\").sample(n=1, random_state=42)\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "sampled_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test[\"species\"] = full_test[\"species\"].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_species_test = full_test[full_test[\"species\"].isna()][\"species\"].unique()\n",
    "print(\"Lo√†i ch∆∞a √°nh x·∫°:\", unknown_species_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = full_test.drop(columns=[\"path\", \"species\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_multi_test = voting_clf.predict_proba(X_test)\n",
    "pred_multi_test = voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = full_test[\"species\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# T√≠nh Accuracy\n",
    "accuracy = accuracy_score(y_test_true, pred_multi_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# In b√°o c√°o chi ti·∫øt Precision, Recall, F1-score\n",
    "print(classification_report(y_test_true, pred_multi_test, digits=4))\n",
    "\n",
    "# V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test_true,\n",
    "    y_pred=pred_multi_test,\n",
    "    normalize=\"true\",  # B√¨nh th∆∞·ªùng h√≥a theo h√†ng (gi√° tr·ªã t·ª´ 0 -> 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·∫•y t√™n file t·ª´ c·ªôt \"path\", b·ªè ph·∫ßn m·ªü r·ªông .las\n",
    "file_names = full_test[\"path\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "# T·∫°o DataFrame submission\n",
    "submission = pd.DataFrame({\n",
    "    \"name\": file_names,\n",
    "    \"label\": pred_multi_test\n",
    "})\n",
    "\n",
    "# L∆∞u file CSV\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"File submission.csv ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "df_sub.info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
