{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install laspy\n",
    "!pip install rasterio\n",
    "!pip install alphashape shapely\n",
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import tqdm\n",
    "import geopandas as gpd\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, RocCurveDisplay\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import laspy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"/kaggle/input/hutechaichallenge2024-mc/Train/*/*.las\")\n",
    "file_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"path\": file_paths})\n",
    "data[\"species\"] = data[\"path\"].map(lambda p: p.split(\"/\")[-2])\n",
    "data.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the highest tree (Ptop)\n",
    "def find_highest_tree(points):\n",
    "    idx = np.argmax(points[:, 2])  # Find point with max Z\n",
    "    return points[idx]\n",
    "\n",
    "# Step 2: Rotate profile and extract edge points\n",
    "def extract_tree_edges(points, Ptop, num_segments=36):\n",
    "    angles = np.linspace(0, np.pi, num_segments)  # Rotate from 0° to 180°\n",
    "    edge_points = []\n",
    "    \n",
    "    for angle in angles:\n",
    "        # Rotate points around Ptop\n",
    "        rot_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "        rotated_xy = (points[:, :2] - Ptop[:2]) @ rot_matrix.T + Ptop[:2]\n",
    "        rotated_points = np.column_stack((rotated_xy, points[:, 2]))\n",
    "        \n",
    "        # Find the convex hull\n",
    "        hull = ConvexHull(rotated_points[:, :2])\n",
    "        edge_points.append(rotated_points[hull.vertices])\n",
    "    \n",
    "    return edge_points\n",
    "\n",
    "# Step 3: Cluster trees based on extracted edges\n",
    "def segment_trees(points, edge_points, threshold=2.0):\n",
    "    clustered = np.zeros(len(points), dtype=int)\n",
    "    cluster_id = 1\n",
    "    \n",
    "    for edges in edge_points:\n",
    "        for edge in edges:\n",
    "            distances = np.linalg.norm(points[:, :2] - edge[:2], axis=1)\n",
    "            clustered[distances < threshold] = cluster_id\n",
    "        cluster_id += 1\n",
    "    \n",
    "    return clustered.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tree_crown_keypoints(points, tree_top, dis):\n",
    "    \"\"\"\n",
    "    Extract keypoints of the tree crown based on the steps provided.\n",
    "\n",
    "    Args:\n",
    "        points (numpy.ndarray): Array of points (N, 3) representing the point cloud.\n",
    "        tree_top (tuple): (x, y, z) coordinates of the tree top vertex.\n",
    "        dis (float): Average point spacing distance.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Keypoints of the tree crown.\n",
    "    \"\"\"\n",
    "    # 1. Draw the vertical line L from the tree top to the ground\n",
    "    x_top, y_top, z_top = tree_top\n",
    "    z_min = np.min(points[:, 2])  # Ground level (minimum Z value)\n",
    "    \n",
    "    # 2. Calculate the width D\n",
    "    D = 2 * dis  # Width of the profile around the tree top\n",
    "    \n",
    "    # 3. Draw parallel lines with spacing d\n",
    "    d = dis  # Can use the same value for spacing\n",
    "    keypoints = []\n",
    "\n",
    "    # Iterate through several levels from tree top to ground\n",
    "    num_lines = int((z_top - z_min) / d)  # Number of lines along the Z-axis\n",
    "    for i in range(num_lines):\n",
    "        z_level = z_top - i * d  # Current level of the parallel line\n",
    "        # Find points around the current Z level within the profile width D\n",
    "        points_at_level = points[(points[:, 2] >= z_level - D / 2) & \n",
    "                                  (points[:, 2] <= z_level + D / 2) & \n",
    "                                  (np.abs(points[:, 0] - x_top) <= D / 2) & \n",
    "                                  (np.abs(points[:, 1] - y_top) <= D / 2)]\n",
    "        \n",
    "        # Collect keypoints\n",
    "        keypoints.append(points_at_level)\n",
    "    \n",
    "    keypoints = np.vstack(keypoints) if len(keypoints) > 0 else np.array([])\n",
    "    return keypoints\n",
    "\n",
    "def plot_tree_crown(points, keypoints, tree_top):\n",
    "    \"\"\"\n",
    "    Plot the points and keypoints of the tree crown.\n",
    "\n",
    "    Args:\n",
    "        points (numpy.ndarray): Original point cloud.\n",
    "        keypoints (numpy.ndarray): Keypoints of the tree crown.\n",
    "        tree_top (tuple): (x, y, z) coordinates of the tree top vertex.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot all points\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=1, label=\"Point Cloud\")\n",
    "    \n",
    "    # Plot keypoints\n",
    "    ax.scatter(keypoints[:, 0], keypoints[:, 1], keypoints[:, 2], color='red', s=10, label=\"Tree Crown Keypoints\")\n",
    "    \n",
    "    # Mark the tree top\n",
    "    ax.scatter(tree_top[0], tree_top[1], tree_top[2], color='green', s=50, label=\"Tree Top\")\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend()\n",
    "    plt.title(\"Tree Crown Keypoints Extraction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphashape\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import open3d as o3d\n",
    "\n",
    "def compute_alpha_shape(point_cloud, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Dựng biên của đám mây điểm bằng thuật toán Alpha Shapes.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (numpy.ndarray): Đám mây điểm (N, 3) chứa (x, y, z).\n",
    "        alpha (float): Giá trị alpha để điều chỉnh độ chặt chẽ của biên.\n",
    "\n",
    "    Returns:\n",
    "        Polygon hoặc MultiPolygon: Đường biên của tán cây.\n",
    "    \"\"\"\n",
    "    # Chỉ lấy tọa độ X, Y (bỏ Z vì biên là 2D)\n",
    "    xy_points = point_cloud[:, :2]\n",
    "\n",
    "    # Tạo Alpha Shape\n",
    "    alpha_shape = alphashape.alphashape(xy_points, alpha)\n",
    "\n",
    "    return alpha_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_alpha_shape(point_cloud, alpha_shape):\n",
    "    \"\"\"\n",
    "    Vẽ đường biên của tán cây dựa trên Alpha Shapes.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (numpy.ndarray): Đám mây điểm (N, 3).\n",
    "        alpha_shape (Polygon hoặc MultiPolygon): Đường biên.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(point_cloud[:, 0], point_cloud[:, 1], s=1, label=\"Điểm\")\n",
    "    \n",
    "    if isinstance(alpha_shape, Polygon):\n",
    "        x, y = alpha_shape.exterior.xy\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label=\"Alpha Shape\")\n",
    "    elif isinstance(alpha_shape, MultiPolygon):\n",
    "        for poly in alpha_shape.geoms:\n",
    "            x, y = poly.exterior.xy\n",
    "            plt.plot(x, y, 'r-', linewidth=2, label=\"Alpha Shape\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(\"Alpha Shape của Tán Cây\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def compute_cloud_features(path: str, *, height_threshold: float = 1.0) -> dict:\n",
    "    las = laspy.read(path)\n",
    "    out = {}\n",
    "    \n",
    "    points = np.stack((las.x, las.y, las.z), axis=1)\n",
    "    \n",
    "    height_mask = points[:, 2] > height_threshold\n",
    "    points = points[height_mask]\n",
    "\n",
    "    # ĐẶc trưng tọa độ\n",
    "    x_min, x_max = points[:, 0].min(), points[:, 0].max()\n",
    "    y_min, y_max = points[:, 1].min(), points[:, 1].max()\n",
    "    z_min, z_max = points[:, 2].min(), points[:, 2].max()\n",
    "    mean_x, mean_y, mean_z = points[:, 0].mean(), points[:, 1].mean(), points[:, 2].mean()\n",
    "    var_x, var_y, var_z = points[:, 0].var(), points[:, 1].var(), points[:, 2].var()\n",
    "\n",
    "    # Bounding Box (Hình chữ nhật)\n",
    "    width = x_max - x_min\n",
    "    depth = y_max - y_min\n",
    "    height = z_max - z_min\n",
    "\n",
    "    # 📌 Shape Ratio\n",
    "    vertical_proportion = float(height) / (width + depth)\n",
    "\n",
    "    # 📌 Mật độ điểm\n",
    "    density_xy = len(points) / ((x_max - x_min) * (y_max - y_min))\n",
    "    density_yz = len(points) / ((y_max - y_min) * (z_max - z_min))\n",
    "    density_xyz = len(points) / ((x_max - x_min) * (y_max - y_min) * (z_max - z_min))\n",
    "    point_density_ratio = density_xy / density_xyz  # 📌 Tỷ lệ mật độ điểm 2D/3D\n",
    "\n",
    "    # 📌 Đặc trưng chiều cao\n",
    "    P10, P25, P50, P75, P95, P99 = np.percentile(points[:, 2], [10, 25, 50, 75, 95, 99])\n",
    "    canopy_height_ratio = (P75 - P50) / (P99 - P25)\n",
    "\n",
    "    # 📌 PCA Eigenvalues (Đánh giá hướng chính)\n",
    "    cov_matrix = np.cov(points.T)\n",
    "    eigenvalues = np.linalg.eigvals(cov_matrix)  # Tính eigenvalues\n",
    "    eigenvalue_sum = np.sum(eigenvalues)  # Tổng tất cả eigenvalues\n",
    "    eigenvalue_ratio = np.max(eigenvalues) / (eigenvalue_sum + 1e-10)  # Hướng chính của tán\n",
    "\n",
    "    # 📌 Tính các đặc trưng dựa trên PCA\n",
    "    if eigenvalue_sum > 0:\n",
    "        linearity = eigenvalues[0] / eigenvalue_sum\n",
    "        scatter = eigenvalues[2] / eigenvalue_sum\n",
    "        eigentropy = -np.sum((eigenvalues / eigenvalue_sum) * np.log(eigenvalues / eigenvalue_sum + 1e-10))\n",
    "    else:\n",
    "        linearity = scatter = eigentropy = 0\n",
    "    \n",
    "    omnivariance = (eigenvalues[0] * eigenvalues[1] * eigenvalues[2]) ** (1/3) if np.all(eigenvalues > 0) else 0\n",
    "    sum_of_eigenvalues = eigenvalue_sum\n",
    "    planarity = (eigenvalues[1] - eigenvalues[2]) / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "    sphericity = eigenvalues[2] / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "    symmetry = eigenvalues[1] / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "\n",
    "    # 📌 Đặc trưng cấu trúc\n",
    "    mean_height = np.mean(points[:, 2]) - z_min\n",
    "    std_height = np.std(points[:, 2])\n",
    "    canopy_ratio = np.sum(points[:, 2] > np.median(points[:, 2])) / len(points)\n",
    "    roughness = np.std(points[:, 2] - mean_z)\n",
    "    normalized_height_ratio = mean_height / (z_max - z_min)\n",
    "\n",
    "    # 📌 Convex Hull (Thể tích & Diện tích)\n",
    "    try:\n",
    "        hull = ConvexHull(points)\n",
    "        convex_hull_volume = hull.volume\n",
    "        convex_hull_area = hull.area\n",
    "        \n",
    "        # 📌 Tính bán kính Bounding Sphere (bán kính lớn nhất từ centroid đến điểm xa nhất)\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        distances = np.linalg.norm(points - centroid, axis=1)\n",
    "        bounding_sphere_radius = np.max(distances)\n",
    "        \n",
    "        # 📌 Tính Hull Compactness\n",
    "        sphere_volume = (4/3) * np.pi * (bounding_sphere_radius ** 3)\n",
    "        hull_compactness = convex_hull_volume / sphere_volume if sphere_volume > 0 else 0\n",
    "    \n",
    "    except:\n",
    "        convex_hull_volume = 0  # Trường hợp không tính được\n",
    "        convex_hull_area = 0\n",
    "        hull_compactness = 0\n",
    "\n",
    "    # 📌 Vertical Distribution (Phân bố điểm theo Z)\n",
    "    hist_z, _ = np.histogram(points[:, 2], bins=10)\n",
    "    vertical_distribution = np.std(hist_z) / np.mean(hist_z) if np.mean(hist_z) > 0 else 0\n",
    "\n",
    "    # 📌 Point Entropy (Độ hỗn loạn của phân bố điểm)\n",
    "    probs = hist_z / np.sum(hist_z) if np.sum(hist_z) > 0 else np.ones(10) / 10\n",
    "    point_entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "\n",
    "    # 📌 Sự phân bố không gian của các đỉnh cao nhất\n",
    "    top_points = points[points[:, 2] > np.percentile(points[:, 2], 90)]\n",
    "    mean_x_top, mean_y_top, mean_z_top = np.mean(top_points, axis=0)\n",
    "    var_x_top, var_y_top, var_z_top = np.var(top_points, axis=0)\n",
    "\n",
    "    # 📌 Tính z_entropy\n",
    "    z_probs = np.histogram(points[:, 2], bins=10, density=True)[0]\n",
    "    z_entropy_value = -np.sum(z_probs * np.log(z_probs + 1e-10))\n",
    "    pct_z_above_mean = np.sum(points[:, 2] > mean_z) / len(points) * 100\n",
    "    pct_z_above_2 = np.sum(points[:, 2] > 2) / len(points) * 100\n",
    "\n",
    "    # 📌 Độ rộng trung bình của tán cây\n",
    "    tree_top = (mean_x, mean_y, z_max)\n",
    "    crown_keypoints = extract_tree_crown_keypoints(points, tree_top, dis=0.5)\n",
    "    num_crown_keypoints = len(crown_keypoints)\n",
    "    avg_crown_width = np.mean(np.linalg.norm(crown_keypoints[:, :2] - np.array(tree_top[:2]), axis=1)) * 2 if num_crown_keypoints > 0 else 0\n",
    "\n",
    "    # 📌 Tính diện tích biên của tán cây\n",
    "    alpha_shape = compute_alpha_shape(points, alpha=0.1)\n",
    "    crown_boundary_area = sum(poly.area for poly in alpha_shape.geoms) if isinstance(alpha_shape, MultiPolygon) else (alpha_shape.area if isinstance(alpha_shape, Polygon) else 0)\n",
    "    \n",
    "    # Height distribution features (Woods et al., 2008)\n",
    "    out.update(\n",
    "        {\n",
    "            \"width\": width, \"depth\": depth, \"height\": height,\n",
    "            \"min_z\": z_min, \"max_z\": z_max, \"mean_z\": mean_z, \"var_z\": var_z,\n",
    "            \"vertical_proportion\": vertical_proportion,\n",
    "            \"density_xy\": density_xy, \"density_yz\": density_yz, \"density_xyz\": density_xyz,\n",
    "            \"P10\": P10, \"P25\": P25, \"P50\": P50, \"P75\": P75, \"P95\": P95, \"P99\": P99, \"canopy_height_ratio\": canopy_height_ratio,\n",
    "            \"eigenvalue_1\": eigenvalues[0], \"eigenvalue_2\": eigenvalues[1], \"eigenvalue_3\": eigenvalues[2], \"eigenvalue_sum\": eigenvalue_sum, \"eigenvalue_ratio\": eigenvalue_ratio,\n",
    "            \"linearity\": linearity, \"scatter\": scatter, \"omnivariance\": omnivariance, \"eigentropy\": eigentropy, \"sum_of_eigenvalues\": sum_of_eigenvalues, \"planarity\": planarity, \"sphericity\": sphericity, \"symmetry\": symmetry,\n",
    "            \"mean_height\": mean_height, \"std_height\": std_height, \"roughness\": roughness,\n",
    "            \"convex_hull_volume\": convex_hull_volume, \"convex_hull_area\": convex_hull_area,\n",
    "            \"vertical_distribution\": vertical_distribution, \"point_entropy\": point_entropy,\n",
    "            \"top_points\": top_points, \"mean_x_top\": mean_x_top, \"mean_y_top\": mean_y_top, \"mean_z_top\": mean_z_top,\n",
    "            \"var_x_top\": var_x_top, \"var_y_top\": var_y_top, \"var_z_top\": var_z_top,\n",
    "            \"z_entropy_value\": z_entropy_value, \"pct_z_above_mean\": pct_z_above_mean, \"pct_z_above_2\": pct_z_above_2,\n",
    "            \"avg_crown_width\": avg_crown_width, \"crown_boundary_area\": crown_boundary_area, \"alpha_shape\": alpha_shape\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.json_normalize(data[\"path\"].map(compute_cloud_features))\n",
    "full = pd.concat([data, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full[\"mean_x_top_points\"] = full[\"top_points\"].apply(lambda pts: np.mean([p[0] for p in pts]))\n",
    "full[\"mean_y_top_points\"] = full[\"top_points\"].apply(lambda pts: np.mean([p[1] for p in pts]))\n",
    "full[\"mean_z_top_points\"] = full[\"top_points\"].apply(lambda pts: np.mean([p[2] for p in pts]))\n",
    "\n",
    "full[\"var_x_top_points\"] = full[\"top_points\"].apply(lambda pts: np.var([p[0] for p in pts]))\n",
    "full[\"var_y_top_points\"] = full[\"top_points\"].apply(lambda pts: np.var([p[1] for p in pts]))\n",
    "full[\"var_z_top_points\"] = full[\"top_points\"].apply(lambda pts: np.var([p[2] for p in pts]))\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "full[\"max_top_distance\"] = full[\"top_points\"].apply(lambda pts: np.max(pdist(pts[:, :2])) if len(pts) > 1 else 0)\n",
    "\n",
    "full[\"crown_boundary_area\"] = full[\"alpha_shape\"].apply(lambda poly: poly.area)\n",
    "full[\"crown_perimeter\"] = full[\"alpha_shape\"].apply(lambda poly: poly.length)\n",
    "full[\"crown_compactness\"] = full[\"crown_boundary_area\"] / full[\"crown_perimeter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy ngẫu nhiên 1 mẫu từ mỗi species\n",
    "sampled_data = full.groupby(\"species\").sample(n=1, random_state=42)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_mapping = {\n",
    "    \"Fir\": 1,      # Cây lãnh sam\n",
    "    \"Pine\": 2,     # Cây thông\n",
    "    \"Spruce\": 3,   # Cây vân sam\n",
    "    \"Alder\": 4,    # Cây trăn\n",
    "    \"Aspen\": 5,    # Cây dương\n",
    "    \"Birch\": 6,    # Cây bạch dương\n",
    "    \"Tilia\": 7     # Cây đoạn\n",
    "}\n",
    "\n",
    "full[\"species\"] = full[\"species\"].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_species = full[full[\"species\"].isna()][\"species\"].unique()\n",
    "print(\"Loài chưa ánh xạ:\", unknown_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full.drop(columns=[\"path\", \"species\"]).copy()\n",
    "y = full.pop(\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=68, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass clssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer trích xuất diện tích từ alpha_shape\n",
    "class AlphaShapeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            shape.area if shape is not None else 0  # Xử lý giá trị None\n",
    "            for shape in X[\"alpha_shape\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# Transformer trích xuất trung bình độ cao từ top_points\n",
    "class TopPointsFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([p[2] for p in points]) if isinstance(points, list) and len(points) > 0 else 0\n",
    "            for points in X[\"top_points\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# ColumnTransformer xử lý tất cả cột số + trích xuất đặc trưng từ object\n",
    "feature_extractor = ColumnTransformer([\n",
    "    (\"scale_numeric\", StandardScaler(), make_column_selector(dtype_include=np.number)),  # Chuẩn hóa tất cả cột số\n",
    "    (\"extract_alpha_shape\", AlphaShapeFeatureExtractor(), [\"alpha_shape\"]),  # Trích xuất từ alpha_shape\n",
    "    (\"extract_top_points\", TopPointsFeatureExtractor(), [\"top_points\"])  # Trích xuất từ top_points\n",
    "])\n",
    "\n",
    "# Pipeline với Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    (\"feature_extraction\", feature_extractor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=10000, \n",
    "                                      multi_class=\"multinomial\", \n",
    "                                      class_weight=\"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer trích xuất diện tích từ alpha_shape\n",
    "class AlphaShapeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            shape.area if shape is not None else 0  # Xử lý giá trị None\n",
    "            for shape in X[\"alpha_shape\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# Transformer trích xuất trung bình độ cao từ top_points\n",
    "class TopPointsFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([p[2] for p in points]) if isinstance(points, list) and len(points) > 0 else 0\n",
    "            for points in X[\"top_points\"]\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "# ColumnTransformer xử lý tất cả cột số + trích xuất đặc trưng từ object\n",
    "feature_extractor = ColumnTransformer([\n",
    "    (\"scale_numeric\", StandardScaler(), make_column_selector(dtype_include=np.number)),  # Chuẩn hóa tất cả cột số\n",
    "    (\"extract_alpha_shape\", AlphaShapeFeatureExtractor(), [\"alpha_shape\"]),  # Trích xuất từ alpha_shape\n",
    "    (\"extract_top_points\", TopPointsFeatureExtractor(), [\"top_points\"])  # Trích xuất từ top_points\n",
    "])\n",
    "\n",
    "# Pipeline với Logistic Regression\n",
    "pipe_rf = Pipeline([\n",
    "    (\"feature_extraction\", feature_extractor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=1000, \n",
    "                           class_weight=\"balanced\", \n",
    "                           random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cái này là giá của luồng pipe_lr\n",
    "cv_result_multi = cross_validate(\n",
    "    estimator=pipe_lr,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "scores = cv_result_multi[\"test_score\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(scores, ls=\"\", marker=\".\", c=\"k\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xlabel(\"Cross-validation iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.axhline(scores.mean(), linestyle=\"dashed\", alpha=0.3, c=\"k\")\n",
    "ax.annotate(\n",
    "    text=f\"Accuracy: {scores.mean():.3f} ± {scores.std():.3f}\",\n",
    "    xy=(4.5, 0.2),\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    bbox={\n",
    "        \"facecolor\": \"white\",\n",
    "    },\n",
    ")\n",
    "ax.set_title(\"Cross-validation results (multiclass)\")\n",
    "ax.grid(color=\"k\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cái này là giá của luồng pipe_rf\n",
    "cv_result_multi = cross_validate(\n",
    "    estimator=pipe_rf,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "scores = cv_result_multi[\"test_score\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(scores, ls=\"\", marker=\".\", c=\"k\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xlabel(\"Cross-validation iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.axhline(scores.mean(), linestyle=\"dashed\", alpha=0.3, c=\"k\")\n",
    "ax.annotate(\n",
    "    text=f\"Accuracy: {scores.mean():.3f} ± {scores.std():.3f}\",\n",
    "    xy=(4.5, 0.2),\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    bbox={\n",
    "        \"facecolor\": \"white\",\n",
    "    },\n",
    ")\n",
    "ax.set_title(\"Cross-validation results (multiclass)\")\n",
    "ax.grid(color=\"k\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện pipeline\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_multi_lr = pipe_lr.predict(X_test)\n",
    "pred_multi_rf = pipe_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Tính Accuracy\n",
    "accuracy = accuracy_score(y_test, pred_multi_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# In báo cáo chi tiết Precision, Recall, F1-score\n",
    "print(classification_report(y_test, pred_multi_lr, digits=4))\n",
    "\n",
    "# Vẽ ma trận nhầm lẫn (Confusion Matrix)\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test,\n",
    "    y_pred=pred_multi_lr,\n",
    "    normalize=\"true\",  # Bình thường hóa theo hàng (giá trị từ 0 -> 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Tính Accuracy\n",
    "accuracy = accuracy_score(y_test, pred_multi_rf)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# In báo cáo chi tiết Precision, Recall, F1-score\n",
    "print(classification_report(y_test, pred_multi_rf, digits=4))\n",
    "\n",
    "# Vẽ ma trận nhầm lẫn (Confusion Matrix)\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test,\n",
    "    y_pred=pred_multi_rf,\n",
    "    normalize=\"true\",  # Bình thường hóa theo hàng (giá trị từ 0 -> 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết hợp 2 mô hình bằng VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', pipe_lr),\n",
    "    ('rf', pipe_rf)\n",
    "], voting='soft')  # 'soft' dùng xác suất dự đoán, 'hard' chỉ lấy kết quả đa số\n",
    "\n",
    "# Train mô hình\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_file_paths = glob.glob(\"/kaggle/input/hutechaichallenge2024-mc/Test/*/*.las\")\n",
    "test_file_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"path\": test_file_paths})\n",
    "test_data[\"species\"] = test_data[\"path\"].map(lambda p: p.split(\"/\")[-2])\n",
    "test_data.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = pd.json_normalize(test_data[\"path\"].map(compute_cloud_features))\n",
    "full_test = pd.concat([test_data, features_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test[\"mean_x_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.mean([p[0] for p in pts]))\n",
    "full_test[\"mean_y_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.mean([p[1] for p in pts]))\n",
    "full_test[\"mean_z_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.mean([p[2] for p in pts]))\n",
    "\n",
    "full_test[\"var_x_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.var([p[0] for p in pts]))\n",
    "full_test[\"var_y_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.var([p[1] for p in pts]))\n",
    "full_test[\"var_z_top_points\"] = full_test[\"top_points\"].apply(lambda pts: np.var([p[2] for p in pts]))\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "full_test[\"max_top_distance\"] = full_test[\"top_points\"].apply(lambda pts: np.max(pdist(pts[:, :2])) if len(pts) > 1 else 0)\n",
    "\n",
    "full_test[\"crown_boundary_area\"] = full_test[\"alpha_shape\"].apply(lambda poly: poly.area)\n",
    "full_test[\"crown_perimeter\"] = full_test[\"alpha_shape\"].apply(lambda poly: poly.length)\n",
    "full_test[\"crown_compactness\"] = full_test[\"crown_boundary_area\"] / full_test[\"crown_perimeter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy ngẫu nhiên 1 mẫu từ mỗi species\n",
    "sampled_data_test = full_test.groupby(\"species\").sample(n=1, random_state=42)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "sampled_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test[\"species\"] = full_test[\"species\"].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_species_test = full_test[full_test[\"species\"].isna()][\"species\"].unique()\n",
    "print(\"Loài chưa ánh xạ:\", unknown_species_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = full_test.drop(columns=[\"path\", \"species\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_multi_test = voting_clf.predict_proba(X_test)\n",
    "pred_multi_test = voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = full_test[\"species\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Tính Accuracy\n",
    "accuracy = accuracy_score(y_test_true, pred_multi_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# In báo cáo chi tiết Precision, Recall, F1-score\n",
    "print(classification_report(y_test_true, pred_multi_test, digits=4))\n",
    "\n",
    "# Vẽ ma trận nhầm lẫn (Confusion Matrix)\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_test_true,\n",
    "    y_pred=pred_multi_test,\n",
    "    normalize=\"true\",  # Bình thường hóa theo hàng (giá trị từ 0 -> 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tên file từ cột \"path\", bỏ phần mở rộng .las\n",
    "file_names = full_test[\"path\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "# Tạo DataFrame submission\n",
    "submission = pd.DataFrame({\n",
    "    \"name\": file_names,\n",
    "    \"label\": pred_multi_test\n",
    "})\n",
    "\n",
    "# Lưu file CSV\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"File submission.csv đã được tạo thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "df_sub.info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
